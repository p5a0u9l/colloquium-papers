## An information theoretic approach to processing management
In my previous paper, which discussed a Cognitive Radar Framework, I expressed the interest in using the framework to investigate the problem of managing finite processing resources in the context of continually increasing sensor data rates. This paper uses concepts from Information Theory to develop the necessary decision parameters needed to instantaneously route sensor data observations to the correct exploitation algorithm.

The information theoretic approach uses the metric of information gain, which can conceptually be described as attenuation of entropy, or uncertainty, in estimates. Of course, in this case the requirement is that the gain must be predicted, and the optimal algorithm selected, otherwise you could simply compute the information gain and select the algorithm which maximized it conditioned on the sensed data.

The metric is computed using the Rényi Divergence, a formula for computing the divergence between two densities $p_1$ and $p_0$. In the context of process management the metric seeks to compute the divergence between the prior density $p(x^k | Z^{k - 1})$ and the _expected_ posterior density when action $r^k$ is taken. The divergence thereby provides a measure of information gain and enables selection of the optimal algorithm with which to process the data.

Given infinite processing resources, the naïve approach to algorithm selection involves testing every hypothesis. That is, for a set of $N$ processing modalities, compute each measurement and choose the answer that minimizes uncertainty. In simple cases, this is often easiest to implement and is certainly the most robust as it makes no assumptions of the data. But, if the algorithm requires extensive processing, e.g. computing an FFT across both dimensions of a 1000 x 1000 matrix, hypothesis testing becomes infeasible for systems constrained by SWaP, as is the case in airborne sensor platforms. If one allows assumptions to be made, then efficiency can be greatly improved by computing the expected posterior densities as mentioned above.

The authors conducted a computer simulation to illustrate and validate the efficacy of information gain as a processing management metric. The simulation used measurements of a ground moving target who moved in both radial range and cross-range. The first dimension is well-suited to Ground Moving Target Indicator (GMTI) processing, while the second is better suited to Synthetic Aperture Radar (SAR) processing. The objective of the simulation was to determine the process management strategy that maximized detection performance given computing resource constraints. In addition to the proposed information theoretic approach, only-SAR, only-GMTI, and rule-based selection approaches were compared. The subject algorithm noticeably outperformed rule-based selection, which itself outperformed the only-... approaches.

Applications for process management are very timely, not only for airborne surveillance sensors, but in the context of Internet of Things and ubiquitous computing, the ability to judiciously apply computing resources is possibly as important as having the latest, fanciest algorithm.

The approach described above makes it's decision at time $k$ using measurements and densities up to $k - 1$, but without regard for future conditions. An extension of this work would involve multi-step prediction. I would also be interested in an experiment using real sensor data and a larger set of algorithms than the two used in the paper.

## Citation
C. Kreucher and K. Carter, "An information theoretic approach to processing management," 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, Las Vegas, NV, 2008, pp. 1869-1872.
